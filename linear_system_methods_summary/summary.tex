\documentclass[9pt, serif]{beamer}

\newlength{\wideitemsep}
\setlength{\wideitemsep}{\itemsep}
%\addtolength{\wideitemsep}{9pt}
\let\olditem\item
\renewcommand{\item}{\setlength{\itemsep}{\wideitemsep}\olditem}

% This file is a solution template for:

% - Talk at a conference/colloquium.
% - Talk length is about 20min.
% - Style is ornate.

\usepackage[noend]{algpseudocode}
\usepackage{graphics}
\usepackage{marvosym}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{amsmath}  
\usepackage{epstopdf}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{helvet}
\usepackage[T1]{fontenc}


\newcommand{\bi}{\begin{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\ee}{\end{enumerate}}
\newcommand*\Let[2]{\State #1 $\gets$ #2}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\norm}[1]{\lVert#1\rVert}

\mode<presentation>
{
% THEME CHOICES
%\usetheme{CambridgeUS}
\usetheme{Warsaw}
%\usetheme{Rochester}
%\usetheme{Bergen}
%\usetheme{Berlin}
%\usetheme{Copenhagen}
%\usetheme{Boadilla}
%\usetheme{PaloAlto}

% SET BACKGROUND COLORS
\setbeamercolor{background canvas}{bg=white}
\setbeamerfont{frametitle}{size=\large}
}
\setbeamercolor{frame}{bg=mygold}


\title[Methods of Solution for Linear Systems]
{A Summary of Methods of Solution for Linear Systems}
\author[]
{Nate DeMaagd, Kurt O'Hearn}
\institute[Grand Valley State University]
{MTH 499-02}
\date{April 23, 2013}


\begin{document}

\begin{frame}
    \titlepage
\end{frame}


\begin{frame}{Outline}
    \pause
    \bi
        \item Types of methods \pause
        \item Direct methods and examples \pause
        \item Indirect methods and examples
    \ei
\end{frame}


%INTRODUCTION
\begin{frame}{Overview of Types of Methods for Solving Linear Systems}
    \pause
    Recall: For the linear system $Ax = b$ with $A_{m\times n}$: \\
    \pause
    \begin{center}
        \begin{tabular}{l|l}
            System Type & Possible Number of Solutions \\ \hline
            Square ($m = n$) & None, Unique \\
            Overdetermined ($m > n$) & None, Unique \\
            Underdetermined ($m < n$) & None, Infinite \\
        \end{tabular}
    \end{center}
    \pause
    Types of Methods
    \bi
        \item Direct methods \pause
        \bi
            \item Execute a predetermined number of computations to produce a result \pause
            \item Methods: compute $A^{-1}$, transform $A$ using factorizations/pivoting \pause
        \ei
        \item Indirect methods \pause
        \bi
            \item Generate a sequence of intermediate results which (hopefully) produce the desired final result \pause
	        \item Methods: \pause
            \bi
                \item General: Richardson, Jacobi, Gauss-Seidel, SOR \pause
                \item Symmetric, positive definite: steepest descent, conjugate gradient \pause
            \ei
    	\ei
    \ei
    Note: methods can be general or exploit certain matrix characteristics
\end{frame}


%GAUSSIAN ELIMINATION OVERVIEW
\begin{frame}{Direct Methods}
    \pause
    Compute $A^{-1}$ \pause
    \bi
        \item Most likely too difficult but not always (e.g., diagonal) \pause
    \ei
    Gaussian Elimination \pause
    \bi
        \item Solve by reducing the coefficient matrix to triangular form using elementary row operations \pause
        \item Result can then by easily solved using forward/backward substitution \pause
        \item Possible row operations:
        \bi
            \item Interchange
            \item Scaling
            \item Addition \pause
        \ei 
        \item Recall: GE is equivalent to computing $LU$ factorization
    \ei
\end{frame}


%GAUSSIAN ELIMINATION: PIVOTING METHODS
\begin{frame}{Direct Methods: Gaussian Elimination and Pivoting Methods}
    Pivoting Methods \pause
    \bi
        \item Types of pivoting:
        \bi
            \item Partial/complete: pertains to the amount of the matrix examined to find where to pivot \pause
            \bi
                \item Partial: analyze one row
                \item Complete: analyze entire submatrix
            \ei \pause
            \item Unscaled/scaled: pertains to how the potential pivot locations are compared \pause
            \bi
                \item Unscaled: compare absolute value of entries directly
                \item Scaled: compare ratios of pivot entries to maximum entries in magnitude in each row
            \ei
        \ei
    \ei
\end{frame}


%PIVOTING EXAMPLE WITH PERMUTATION MATRIX
\begin{frame}{Direct Methods: Pivoting Examples}
    \begin{columns}
    \column{.35\textwidth}
        \only<1-2>{
            $A_1=\begin{bmatrix}
            1 & 3 & 6 \\
            2 & 1 & 1 \\
            1 & 3 & 3
            \end{bmatrix}$
        }
        \only<3-5>{
            $A_2=\begin{bmatrix}
            0 & 2.5 & 5.5 \\
            1 & 0.5 & 0.5 \\
            0 & 2.5 & 2.5
            \end{bmatrix}$
        }
        \only<6-7>{
            $A_3=\begin{bmatrix}
            0 & 1 & 2.2 \\
            1 & 0.5 & $-0.6$ \\
            0 & 0 & $-3$
            \end{bmatrix}$
        }
        \only<8-8>{
            $A_4=\begin{bmatrix}
            0 & 1 & 2.2 \\
            1 & 0.5 & $-0.6$ \\
            0 & 0 & 1
            \end{bmatrix}$
        }
        \vspace{5mm}
        \only<1-1>{
            $p = \begin{bmatrix}1,&2,&3\end{bmatrix}$
        }
        \only<2-8>{
            $p = \begin{bmatrix}2,&1,&3\end{bmatrix}$
        }

    \column{.65\textwidth}
        \begin{algorithmic}[1]
            \Function{GE\_PP}{$n, (a_{ij}), (p_{i})$}
                \For{$k = 1 \textbf{ to } n-1$}
                    \For{$i = k + 1 \textbf{ to } n$}
                        \Let{$z$}{$a_{p_ik}/a_{p_kk}$}
                        \Let{$a_{p_ik}$}{$0$}
                        \For{$j = k + 1 \textbf{ to } n$}
                            \Let{$a_{p_ij}$}{$a_{p_ij}-za_{p_kj}$}
                        \EndFor
                    \EndFor
                \EndFor
                \vspace{-5mm}
                \State \Return{$(a_{ij})$}
            \EndFunction
        \end{algorithmic}
    \end{columns}
    \vspace{8mm}

    \only<1-3>{$$\max(\abs{1},\abs{2},\abs{1}) = 2$$}
    \only<2-3>{$$R_1 \leftarrow R_1 - \frac{1}{2}R_2$$}
    \only<2-3>{$$R_3 \leftarrow R_3 - \frac{1}{2}R_2$$}
    \only<2-3>{$$R_2 \leftarrow \frac{1}{2}R_2$$}
    \only<4-6>{$$\max(\abs{2.5},\abs{2.5}) = 2.5$$}
    \only<5-6>{$$R_3 \leftarrow R_3 - R_2$$}
    \only<5-6>{$$R_1 \leftarrow \frac{2}{5}R_1$$}
    \only<7-8>{$$R_3 \leftarrow -\frac{1}{3}R_3$$}
\end{frame}


\begin{frame}{Iterative Methods}
    \pause
    Recall: General Conditions for Iterative Method Convergence \pause
        \begin{theorem}
        For the linear system $Ax = b$ with $A$ invertible, define the iteration formula $$x^{(k)} = Gx^{(k-1)} + c.$$
        The sequence $\left\{x^{(k)}\right\}$ will converge to $(I - G)^{-1}c$ provided that $\rho(G) < 1$.
    \end{theorem}
    \pause
    Methods
    \bi
        \item Richardson
        \item Jacobi
        \item Gauss-Seidel
        \item SOR
        \item Steepest Descent
        \item Conjugate Gradient
    \ei
\end{frame}


\begin{frame}{Indirect Methods: Richardson}
    Method
    \bi
        \item Richardson
    \ei
    System Stipulations
    \bi
        \item $A$ invertible
    \ei
    Iteration Formula
    \bi
        \item $x^{(k)} = (I-A)x^{(k-1)}+b = x^{(k-1)}+r^{(k-1)}$
    \ei
    Convergence Criteria
    \bi
        \item $\rho(I-A)<1$
    \ei
\end{frame}


%EXAMPLES
\begin{frame}{Indirect Methods: Richardson Method Example}
\pause
\bi
\item Consider matrix $A=\begin{bmatrix}6&1&1\\2&4&0\\1&2&6\end{bmatrix}$ with solution $b=\begin{bmatrix}12\\0\\6\end{bmatrix}.$
\pause
\item Algorithm is $x^{(k+1)} = (I-\omega A)x^{(k)}+\omega b^{(k)}$ for some scalar $\omega\neq 0$ such that $|r|<1$.
\pause
\item Let $x^{(0)} = \begin{bmatrix}2&2&2\end{bmatrix}^T$ and $\omega = \dfrac{1}{6}$.
\pause
\item \begin{align*}
	x^{(1)} &= \begin{bmatrix}0&-1/6&-1/6\\-1/3&1/3&0\\-1/6&-1/3&0\end{bmatrix}\begin{bmatrix}2\\2\\2\end{bmatrix}+\dfrac{1}{6}\begin{bmatrix}12\\0\\6\end{bmatrix}\\
	\vspace{2cm}
	& = \begin{bmatrix}4/3&0&0\end{bmatrix}^T
\end{align*}
\pause
\item After 12 iterations, $x^{(12)}\approx \begin{bmatrix}2&-1&1\end{bmatrix}^T$.
\ei
\end{frame}


\begin{frame}{Indirect Methods: Jacobi}
    Method
    \bi
        \item Jacobi
    \ei
    System Stipulations
    \bi
        \item $A$ invertible
        \item $A$ diagonally dominant
    \ei
    Iteration Formula
    \bi
        \item $x^{(k)} = (I-D^{-1}A)x^{(k-1)}+D^{-1}b$
        \item $D = a_{ii}$
    \ei
    Convergence Criteria
    \bi
        \item $\rho(I-D^{-1}A)<1$
    \ei
\end{frame}


\begin{frame}{Indirect Methods: Gauss-Seidel}
    Method
    \bi
        \item Gauss-Seidel
    \ei
    System Stipulations
    \bi
        \item $A$ invertible
        \item $A$ diagonally dominant
    \ei
    Iteration Formula
    \bi
        \item $x^{(k)} = (I-L^{-1}A)x^{(k-1)}+L^{-1}b$
        \item $L = a_{ij}, i \ge j$ 
    \ei
    Convergence Criteria
    \bi
        \item $\rho(I-L^{-1}A)<1$
    \ei
\end{frame}


\begin{frame}{Indirect Methods: Successive Overrelaxation}
    Method
    \bi
        \item SOR
    \ei
    System Stipulations
    \bi
        \item Complex system
        \item $A$: positive definite, Hermitian
    \ei
    Iteration Formula
    \bi
        \item $x^{(k+1)}=(I-(\alpha D-C)^{-1}A)x^{(k)}+(\alpha D-C)^{(-1)}b$
        \item $D$: positive definite, Hermitian
        \item $C: C+C^*=D-A$
        \item $\alpha\in\mathbb{R} \ni \alpha>\frac{1}{2}$
    \ei
    Convergence Criteria
    \bi
        \item $\rho(I-(\alpha D-C)^{-1}A)<1$
    \ei
    Typical values
    \bi
        \item $D = a_{ii}, C = -a_{ij} \ni i \ge j$
    \ei
\end{frame}


\begin{frame}{Indirect Methods: Steepest Descent}
    Method
    \bi
        \item Steepest Descent
    \ei
    System Stipulations
    \bi
        \item $A$ invertible
        \item $A$ symmetric, positive definite
    \ei
    Iteration Formula
    \bi
        \item $x^{(k)} = x^{(k-1)}+t_{k-1}v^{(k-1)}$
        \item $t_{k-1} = \frac{\langle v^{(k-1)},\, b - Ax^{(k-1)}\rangle}{\langle v^{(k-1)},\, Av^{(k-1)} \rangle}$
        \item $v^{(k-1)} = -\frac{d}{dx} q(x^{(k-1)})$
        \item $q(x) = \langle x^,\, Ax \rangle - 2 \langle x,\, b \rangle$
    \ei
\end{frame}


\begin{frame}{Indirect Methods: Conjugate Gradient}
    Method
    \bi
        \item Conjugate Gradient
    \ei
    System Stipulations
    \bi
        \item $A$ invertible
        \item $A$ symmetric, positive definite
    \ei
    Iteration Formula
    \bi
        \item $x^{(k)} = x^{(k-1)}+t_{k-1}v^{(k-1)}$
        \item $t_{k-1} = \frac{\langle v^{(k-1)},\, b - Ax^{(k-1)}\rangle}{\langle v^{(k-1)},\, Av^{(k-1)} \rangle}$
        \item $\left\{v^{(0)},\ldots,v^{(n)}\right\}$ are $A$-orthogonal
        \item $v^{(k-1)} = -\frac{d}{dx} q(x^{(k-1)})$
        \item $q(x) = \langle x^,\, Ax \rangle - 2 \langle x,\, b \rangle$
    \ei
\end{frame}

\end{document}
